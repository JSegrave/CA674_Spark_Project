{"cells":[{"cell_type":"markdown","source":["# Getting Started: Load up wikipedia hourly log files\n\nWikipedia keeps hourly log files counting how many times every page on wikipedia was viewed in that hour page.\n\nThis code loads all available logfiles for Nov 13th 2015 into Spark for processing.  The available logfiles are identified as being between 'hour_of_first_logfile' and 'hour_of_last_logfile' below (e.g. between 20hrs and 23hrs)"],"metadata":{}},{"cell_type":"code","source":["# Create a SparkContext, if one doesn't already exist\nimport pyspark\nif 'sc' not in locals(): sc = pyspark.SparkContext()\n\n# handy function for looking at load times\nimport time\ndef print_elapsed(start_time): print (\"[elapsed time: %.2f s]\" % (time.time() - start_time))\nstart_time = time.time() # time how long this takes\n\n#filenameformat = './pagecounts-20151113-%02d0000.gz'                    # AWS\n#filenameformat = '/resources/pagecounts-20151113-%02d0000.gz'           # datascientistworkbench\nfilenameformat = 'dbfs:/tmp/pagecounts-20151113-%02d0000.gz' # databricks\n\nhour_of_first_logfile = 0\nhour_of_last_logfile  = 23\n\n# Optimisation: Ignore pages with very low numbers of views per hour, there are millions of them (e.g. pages\n# with 1 view), but they add nothing more than a contribution to the overall total page views.\n# 50 is a resonable value. In low-resource environments, 100 is OK (minimal effect on the results)\nviews_threshold = 50\npagename_representing_filtered_page_counts = 'less_than_threshold'\n\n# load in the raw wikipedia logfiles in parallel across the cluster\n# Then parse the raw text lines of text and add an hour field into the data (so we can filter on time).\n# Must .cache() these RDDs to materialise them. Otherwise, all the RDDs end up with the same hour: 23\nrawRDDs = []\nrawRDDs = [sc.textFile(filenameformat % (hour)) \\\n           .map(lambda rawTextLine: (hour, rawTextLine.split(\" \"))) \\\n           .map(lambda hour_tokens: ((hour_tokens[0], hour_tokens[1][0], hour_tokens[1][1]), int(hour_tokens[1][2]))) \\\n           .cache() \\\n           for hour in range(hour_of_first_logfile, hour_of_last_logfile + 1)]\n\n# Now combine the raw RDDs into one logical one (Spark will still distribute the processing close to the data)\nrawRDD = sc.union(rawRDDs)\ntotalEntries = rawRDD.count() # count the total number of log entries being processed. NB: count here *before* the speed optimisation filter\n\n# print an example raw text line for context\nrawLogSample = rawRDD.take(1)\nprint (\"The pre-processed log entry format is [(<hour>, <language>, <page>), <page_views>] e.g. %s\" % (rawLogSample))\n\n# Split the pages we're more/less interested in (those with more/less views than our chosen threshold).\n# Amalgamate pages we're less interested in (see note above). This preserves the validity of subsequent\n# counts, without continually re-processing millions of low-view pages\nlessThanThresholdRDD = rawRDD.filter(lambda key_views: key_views[1] < views_threshold) \\\n    .map(lambda key_views: ((key_views[0][0], key_views[0][1], pagename_representing_filtered_page_counts), key_views[1])) \\\n    .reduceByKey(lambda x, y: x + y)\nmoreThanThresholdRDD = rawRDD.filter(lambda key_views: key_views[1] >= views_threshold)\n\n# Add the regular and amalgamated pages back together\nlogfileEntriesRDD = sc.union([moreThanThresholdRDD, lessThanThresholdRDD])\nlogfileEntriesRDD.cache()\ntotalViews = logfileEntriesRDD.map(lambda key_views: key_views[1]).sum() # count the total number of page views represented in the log entries\n\nprint_elapsed(start_time)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# Count the page views by language and selected time range.\n* Filter log entries by language and time period.\n* Each page will have multiple view counts (one per hour).\n* Compute the total view count for each page across the time period."],"metadata":{}},{"cell_type":"code","source":["def countPageViewsByLanguageAndTimeRange(rdd, language, minhour, maxhour, printText=False):\n    \n    # filter off pages belonging to our language (note can have en, en.mobile, etc)\n    perLanguageRDD = rdd.filter(lambda key_views: ((key_views[0][1] == language) or (key_views[0][1].startswith(language + '.'))))\n    # add up all the counts across different hours and representations of the language\n    totalPerLanguage = perLanguageRDD.map(lambda key_views: key_views[1]).sum()\n    \n    # filter off pages in our time range\n    perLanguagePerPeriodRDD = perLanguageRDD.filter(lambda key_views: ((key_views[0][0] >= minhour) and (key_views[0][0] <= maxhour)))\n    perLanguagePerPeriodRDD.cache()\n    # add up all the counts in this specific time range\n    totalPerPeriod = perLanguagePerPeriodRDD.map(lambda key_views: key_views[1]).sum()\n\n    # Sum the hourly page counts by page\n    if printText: print ('\\nSumming hourly page counts by page...\\n\\n')\n    pageViewsRDD = perLanguagePerPeriodRDD \\\n        .map(lambda key_views: ((key_views[0][2]), key_views[1])) \\\n        .reduceByKey(lambda x, y: x + y)\n    pageViewsRDD.cache()\n\n    # Print stats, so user can see what's happening\n    if printText: print ('#################################################################################')\n    print ('Total wikipedia page views on Friday Nov 13th                 : %.1f MILLION (from %.1f million raw log entries)' % (totalViews / 1000000.0, totalEntries / 1000000.0))\n    print ('Total page views for language \\'%s\\'                            : %.1f MILLION ' % (language, (totalPerLanguage / 1000000.0)))\n    print ('Number of page views for language \\'%s\\' (%.2f - %.2f hours)  : %.1f MILLION ' % (language, minhour, maxhour, (totalPerPeriod / 1000000.0)))\n\n    return pageViewsRDD\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Find the top 20 pages viewed"],"metadata":{}},{"cell_type":"code","source":["def top20(rdd, language, minhour, maxhour, printText=False):\n    start_time = time.time()\n    # Start by filtering down to the appropriate language and time range \n    pageViewsRDD = countPageViewsByLanguageAndTimeRange(rdd, language, minhour, maxhour, printText)\n\n    # Sort the pages by count, so we can identify the top 20\n    # (not as inefficient as it looks - Spark will optimise it)\n    pageViewsSortedRDD = pageViewsRDD.sortBy(lambda key_views: key_views[1], ascending=False)\n    pageViewsSortedRDD.cache()\n\n    # Get the top 20 pages\n    # Note: There are often a lot of 'pages' in the top 20 that don't have meaning to regular users\n    # e.g. analytics scripts, the landing page, etc. So we'll filter some of these off. This is\n    # not inteded to be comprehensive, just a quick job to make the Nov 13th results informative\n    top20List = pageViewsSortedRDD \\\n        .filter(lambda page_views: page_views[0] not in ['Main_Page', '', 'index.html', language, pagename_representing_filtered_page_counts]) \\\n        .filter(lambda page_views: ':' not in page_views[0]) \\\n        .filter(lambda page_views: '-webkit-linear-gradient' not in page_views[0]) \\\n        .take(20)\n    if printText:\n        print ('######################## TOP 20 PAGES BY NUMBER OF VIEWS ########################')\n        for line in top20List: print ('\\t%s:\\t%s' % (line[1], line[0]))\n        print ('#################################################################################')\n\n    # Print them in a simple bar chart\n    pages = [i[0][0:21] for i in top20List][::-1]\n    views = [i[1] for i in top20List][::-1]\n    if (not printText):\n      %matplotlib inline\n      import matplotlib\n      import matplotlib.pyplot as plt\n      import numpy as np\n      plt.rcParams[\"figure.figsize\"] = [8.0, 6]\n      y_pos = np.arange(len(top20List))\n      with plt.style.context('fivethirtyeight'): plt.barh(y_pos, views, alpha=0.4)\n      plt.axes().yaxis.grid(False) # no horizontal lines\n      plt.yticks(y_pos + 0.5, pages)\n      plt.xlabel('\\nPage Views')\n      plt.title('Top 20 \\'%s\\' page views on Wikipedia (Nov 13th %.2f-%.2f UTC)\\n' % (language, minhour, maxhour))\n      plt.show()\n\n    # Return the top 20\n    print_elapsed(start_time)\n    return top20List"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["# The Goal: Interactive analysis of the top 20 pages\n\nCan now do interactive analysis of these log entries. Select a language and a time period to find out what the top 20 pages viewed in that time period were.\n\nWe picked Nov 13th as the sad events in Paris can be seen in the results - look at the top 20 'fr' pages from 20 to 21 hours (i.e. 8-9pm). Then look again from 22 to 23 (ie 10-11pm) and you can see the effect of the events in Paris"],"metadata":{}},{"cell_type":"markdown","source":["### French top 20, 9 & 10pm Nov 13th"],"metadata":{}},{"cell_type":"code","source":["\ntop20List = top20(logfileEntriesRDD, 'fr', 21, 22, True)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### French top 20, 10 & 11pm Nov 13th"],"metadata":{}},{"cell_type":"code","source":["\ntop20List = top20(logfileEntriesRDD, 'fr', 22, 23, True)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### English top 20, 9 & 10pm Nov 13th"],"metadata":{}},{"cell_type":"code","source":["\ntop20List = top20(logfileEntriesRDD, 'en', 20, 21, True)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### English top 20, 10 & 11pm Nov 13th"],"metadata":{}},{"cell_type":"code","source":["\ntop20List = top20(logfileEntriesRDD, 'en', 22, 23, True)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### English top 20, full day Nov 13th"],"metadata":{}},{"cell_type":"code","source":["\ntop20List = top20(logfileEntriesRDD, 'en', hour_of_first_logfile, hour_of_last_logfile, True)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["# Play with the language and time range here"],"metadata":{"collapsed":false}},{"cell_type":"code","source":["\ntop20List = top20(logfileEntriesRDD, 'en', 22, 23, True)\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":19},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":20}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3.0},"version":"3.4.3","nbconvert_exporter":"python","file_extension":".py"},"name":"Interactively_Analyse_Wikipedia_Page_View_Logs_DATABRICKS","notebookId":4062},"nbformat":4,"nbformat_minor":0}
